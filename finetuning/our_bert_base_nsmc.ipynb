{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BQNcT1aS54q"
      },
      "source": [
        "# KSL-BERT NSMC Finetuning Code\n",
        "[SoongsilBERT NSMC Finetuning 학습 예제](https://colab.research.google.com/drive/1Js24ps3JvsN-WO9DURzueTUeCmg_BP-g?usp=sharing)를 참고하여 제작하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRD-xZBVq0DM",
        "outputId": "8bbb590e-e202-4ebb-b5c7-02ce4f6dcf5d"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/twigfarm/letr-sol-profanity-filter.git\n",
        "%cd letr-sol-profanity-filter/finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWQKIvfIQ-RQ",
        "outputId": "0f644a8c-f5fb-4fa8-cd67-db344a046bd7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==3.5.1\n",
        "!pip install attrDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1oSmmhkIm0n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from attrdict import AttrDict\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    \n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiyNLNmyL6Ru"
      },
      "source": [
        "# KSL-BERT Model Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOskDngPoMZH"
      },
      "source": [
        "## Model Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mza4wIHqpVIX"
      },
      "outputs": [],
      "source": [
        "args = AttrDict({\n",
        "  \"task\": \"nsmc\",\n",
        "  \"data_dir\": \"data\",\n",
        "  \"ckpt_dir\": \"ckpt\",\n",
        "  \"train_file\": \"ratings_train.txt\",\n",
        "  \"dev_file\": \"\",\n",
        "  \"test_file\": \"ratings_test.txt\",\n",
        "  \"evaluate_test_during_training\": True,\n",
        "  \"eval_all_checkpoints\": True,\n",
        "  \"save_optimizer\": False,\n",
        "  \"do_lower_case\": False,\n",
        "  \"do_train\": True,\n",
        "  \"do_eval\": True,\n",
        "  \"max_seq_len\": 128,\n",
        "  \"num_train_epochs\": 10,\n",
        "  \"weight_decay\": 0.0,\n",
        "  \"gradient_accumulation_steps\": 1,\n",
        "  \"adam_epsilon\": 1e-8,\n",
        "  \"warmup_proportion\": 0,\n",
        "  \"max_steps\": -1,\n",
        "  \"max_grad_norm\": 1.0,\n",
        "  \"no_cuda\": False,\n",
        "  \"model_type\": \"bert\",\n",
        "  \"model_name_or_path\": \"dobbytk/KSL-BERT\",\n",
        "  \"output_dir\": \"./finetuning/our-bert-base-nsmc\",\n",
        "  \"seed\": 42,\n",
        "  \"train_batch_size\": 32,\n",
        "  \"eval_batch_size\": 128,\n",
        "  \"logging_steps\": 2000,\n",
        "  \"save_steps\": 2000,\n",
        "  \"learning_rate\": 1e-5\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud2fduUcaQKd"
      },
      "outputs": [],
      "source": [
        "# GPU or CPU\n",
        "GPU_NUM = 1\n",
        "\n",
        "device = torch.device(f'cuda:{GPU_NUM}') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.cuda.set_device(device)\n",
        "\n",
        "print(\"Current cuda device\", torch.cuda.current_device())\n",
        "\n",
        "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "print(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LIJvUuyoMjQ"
      },
      "source": [
        "# Model Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBwRNnOQnZBL"
      },
      "outputs": [],
      "source": [
        "label_list = [\"0\", \"1\"]\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    num_labels=2,\n",
        "    id2label={str(i): label for i, label in enumerate(label_list)},\n",
        "    label2id={label: i for i, label in enumerate(label_list)},\n",
        "    )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    args.model_name_or_path, \n",
        "    do_lower_case=args.do_lower_case\n",
        "    )\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    args.model_name_or_path,\n",
        "    config=config\n",
        "    )\n",
        "model.to(args.device)\n",
        "print(\"Finish\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZORl6KZsMAKL"
      },
      "source": [
        "# NSMC Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMnMNes2wLgZ"
      },
      "source": [
        "학습을 위해서 Text File을 `Dataset`형태로 변환하는 부분입니다.\n",
        "\n",
        "이 과정은 간단하게 3단계로 나뉘어집니다.\n",
        "1. Text File로부터 Text 읽어오기\n",
        "2. Text, Label로 분리하기\n",
        "3. Tokenizer로 text를 학습에 사용할 수 있는 형태로 바꿈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lyacSbNPnJcF"
      },
      "outputs": [],
      "source": [
        "def load_data(tokenizer, mode):\n",
        "  \n",
        "  print(f\"Creating features from dataset file at '{args.data_dir}'\")\n",
        "  # 1. Read file\n",
        "  file_to_read = None\n",
        "  if mode == \"train\":\n",
        "    file_to_read = args.train_file\n",
        "  elif mode == \"test\":\n",
        "    file_to_read = args.test_file\n",
        "  elif mode == \"dev\":\n",
        "    file_to_read = args.dev_file\n",
        "  \n",
        "  input_path = os.path.join(args.data_dir, args.task, file_to_read)\n",
        "  print(f\"LOOKING AT {input_path}\")\n",
        "  with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = []\n",
        "    for line in f:\n",
        "      lines.append(line.strip())\n",
        "\n",
        "  # 2. Split line to data\n",
        "  texts = []\n",
        "  label_map = {label: i for i, label in enumerate(label_list)}\n",
        "  labels = []\n",
        "  for (i, line) in enumerate(lines[1:]):\n",
        "    line = line.split(\"\\t\")\n",
        "    if len(line) != 3:\n",
        "      print(f\"Error {line}\")\n",
        "      continue\n",
        "    text_a = line[1]\n",
        "    if text_a == \"\":\n",
        "      continue    \n",
        "    if i % 10000 ==0:\n",
        "      print(f\"[{i}] {line}\")\n",
        "\n",
        "    # Text Data\n",
        "    texts.append({\n",
        "        \"text_a\": text_a\n",
        "    })\n",
        "    # Label\n",
        "    labels.append(label_map[line[2]])\n",
        "\n",
        "  # 3. Convert text data to feature\n",
        "  batch_encoding = tokenizer.batch_encode_plus(\n",
        "      [(text[\"text_a\"]) for text in texts],\n",
        "      max_length=args.max_seq_len,\n",
        "      padding=\"max_length\",\n",
        "      add_special_tokens=True,\n",
        "      truncation=True,\n",
        "  )\n",
        "\n",
        "  features = []\n",
        "  for i in range(len(texts)):\n",
        "    input = {k: batch_encoding[k][i] for k in  batch_encoding}\n",
        "    if \"token_type_ids\" not in input:\n",
        "      input[\"token_type_ids\"] = [0] * len(input[\"input_ids\"])\n",
        "    features.append(input)\n",
        "\n",
        "  for i, feature in enumerate(features[:5]):\n",
        "    print(\"*** Example ***\")\n",
        "    print(\"input_ids: {}\".format(\" \".join([str(x) for x in feature[\"input_ids\"]])))\n",
        "    print(\"attention_mask: {}\".format(\" \".join([str(x) for x in feature[\"attention_mask\"]])))\n",
        "    print(\"token_type_ids: {}\".format(\" \".join([str(x) for x in feature[\"token_type_ids\"]])))\n",
        "    print(\"label: {}\".format(labels[i]))\n",
        "\n",
        "  # Convert feature to dataset\n",
        "  all_input_ids = torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long)\n",
        "  all_attention_mask = torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
        "  all_token_type_ids = torch.tensor([f[\"token_type_ids\"] for f in features], dtype=torch.long)\n",
        "  all_labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMClHW3w-jj-"
      },
      "source": [
        "`train_dataset`은 train용 데이터셋\n",
        "\n",
        "`test_dataset`은 evaluate용 데이터셋\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuuZYQ6LnMRK",
        "outputId": "8aea4917-2cd8-4a04-f14f-dd9131b1d94e"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_data(tokenizer, \"train\")\n",
        "test_dataset = load_data(tokenizer, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnlY0dW_fiKQ"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCltkMfIX2oo"
      },
      "source": [
        "특정 Dataset의 accuracy를 확인하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vkeQRinBDMWy"
      },
      "outputs": [],
      "source": [
        "def evaluate(_model, _eval_dataset, mode, _global_step=None):\n",
        "  results = {}\n",
        "  eval_sampler = SequentialSampler(_eval_dataset)\n",
        "  eval_dataloader = DataLoader(_eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "  # Eval!\n",
        "  if _global_step != None:\n",
        "    print(f\"***** Running evaluation on {mode} dataset ({_global_step} step) *****\")\n",
        "  else:\n",
        "    print(f\"***** Running evaluation on {mode} dataset *****\")\n",
        "  print(f\"  Num examples = {len(_eval_dataset)}\")\n",
        "  print(f\"  Eval Batch size = {args.eval_batch_size}\")\n",
        "  eval_loss = 0.0\n",
        "  nb_eval_steps = 0\n",
        "  preds = None\n",
        "  out_label_ids = None\n",
        "\n",
        "  # Dataloader [for]\n",
        "  for batch in progress_bar(eval_dataloader):\n",
        "    _model.eval()\n",
        "    batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      inputs = {\n",
        "          \"input_ids\": batch[0],\n",
        "          \"attention_mask\": batch[1],\n",
        "          \"labels\": batch[3]\n",
        "      }\n",
        "      if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "        inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "      outputs = _model(**inputs)\n",
        "      tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "      eval_loss += tmp_eval_loss.mean().item()\n",
        "    nb_eval_steps += 1\n",
        "    if preds is None:\n",
        "      preds = logits.detach().cpu().numpy()\n",
        "      out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "    else:\n",
        "      preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "      out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "  # End Dataloader [for]\n",
        "\n",
        "  eval_loss = eval_loss / nb_eval_steps\n",
        "  preds = np.argmax(preds, axis=1)\n",
        "  result = {\n",
        "      \"acc\": (out_label_ids == preds).mean()\n",
        "  }\n",
        "  results.update(result)\n",
        "\n",
        "  output_dir = os.path.join(args.output_dir, mode)\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  output_eval_file = os.path.join(output_dir, f\"{mode}-{_global_step}.txt\" if _global_step else f\"{mode}.txt\")\n",
        "  with open(output_eval_file, \"w\") as f_w:\n",
        "    print(f\"***** Eval results on {mode} dataset *****\")\n",
        "    for key in sorted(results.keys()):\n",
        "      print(f\"  {key} = {str(results[key])}\")\n",
        "      f_w.write(f\"  {key} = {str(results[key])}\\n\")\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zsq5ck10fj7O"
      },
      "outputs": [],
      "source": [
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nsFhOntE8CBI"
      },
      "outputs": [],
      "source": [
        "if args.max_steps > 0:\n",
        "  t_total = args.max_steps\n",
        "  args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "else:\n",
        "  t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQB8rkJyvfBN"
      },
      "outputs": [],
      "source": [
        "global_step = 1\n",
        "epochs_trained = 0\n",
        "steps_trained_in_current_epoch = 0\n",
        "\n",
        "if os.path.exists(args.output_dir):\n",
        "  try:\n",
        "    print(\"  Find latest checkpoint\")\n",
        "    ckpts_suffix = [int(c.split(\"-\")[-1]) for c in os.listdir(args.output_dir) if c.startswith(\"checkpoint\")] \n",
        "    global_step = max(ckpts_suffix)\n",
        "    ckpt_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\" )\n",
        "    \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
        "    model.to(args.device)\n",
        "\n",
        "    epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "    steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "    print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "    print(f\"  Continuing training from epoch {epochs_trained}\")\n",
        "    print(f\"  Continuing training from global step {global_step}\", )\n",
        "    print(f\"  Will skip the first {steps_trained_in_current_epoch} steps in the first epoch\")\n",
        "  except ValueError:\n",
        "    print(\"  Starting fine-tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7VAaK8df866w"
      },
      "outputs": [],
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "  {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "   'weight_decay': args.weight_decay},\n",
        "  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n",
        "   'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHmAkLOyvS6x"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n23B9d9x_Pan",
        "outputId": "48777697-b139-4f38-9bce-832c7b588867"
      },
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Traning model_ = {args.model_name_or_path}\")\n",
        "print(f\"  Num examples = {len(train_dataset)}\")\n",
        "print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "print(f\"  Total train batch size = {args.train_batch_size}\")\n",
        "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {t_total}\")\n",
        "print(f\"  Logging steps = {args.logging_steps}\")\n",
        "print(f\"  Save steps = { args.save_steps}\")\n",
        "\n",
        "tr_loss = 0.0\n",
        "model.zero_grad()\n",
        "mb = master_bar(range(int(args.num_train_epochs)))\n",
        "for epoch in mb:\n",
        "  if epochs_trained > 0:\n",
        "    epochs_trained -= 1\n",
        "    continue\n",
        "\n",
        "  epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "  # One epoch train\n",
        "  for step, batch in enumerate(epoch_iterator):\n",
        "    # Skip past any already trained steps if resuming training\n",
        "    if steps_trained_in_current_epoch > 0:\n",
        "      steps_trained_in_current_epoch -= 1\n",
        "      continue\n",
        "\n",
        "    model.train()\n",
        "    batch = tuple(t.to(args.device) for t in batch)\n",
        "    inputs = {\n",
        "        \"input_ids\":batch[0],\n",
        "        \"attention_mask\": batch[1],\n",
        "        \"labels\": batch[3]\n",
        "    }\n",
        "\n",
        "    if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
        "      inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if args.gradient_accumulation_steps > 1:\n",
        "      loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "    loss.backward()\n",
        "    tr_loss += loss.item()\n",
        "    # Accumulation \n",
        "    if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
        "        len(train_dataloader) <= args.gradient_accumulation_steps \n",
        "        and (step + 1) == len(train_dataloader)\n",
        "    ):\n",
        "      torch.nn.utils.clip_grad_norm(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      model.zero_grad()\n",
        "      global_step += 1\n",
        "\n",
        "      # Evaluate \n",
        "      if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "        if args.evaluate_test_during_training:\n",
        "          evaluate(model, test_dataset, mode=\"test\", _global_step=global_step)\n",
        "        else:\n",
        "          evaluate(model, dev_dataset, mode=\"dev\", _global_step=global_step)\n",
        "      # End Evaluate [if]\n",
        "\n",
        "      # Save model checkpoint\n",
        "      if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "        output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "        if not os.path.exists(output_dir):\n",
        "          os.makedirs(output_dir)\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "        print(f\" Saving model checkpoint to {output_dir}\")\n",
        "\n",
        "        if args.save_optimizer:\n",
        "          torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "          torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "      # End Save model [if]\n",
        "    # End Accumulation [if]\n",
        "    \n",
        "    if args.max_steps > 0 and global_step > args.max_steps:\n",
        "      break\n",
        "  # End One epoch train [for]\n",
        "  mb.write(\"Epoch {} done\".format(epoch + 1))\n",
        "\n",
        "  if args.max_steps > 0 and global_step > args.max_steps:\n",
        "    break \n",
        "tr_loss = tr_loss / global_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blBc0iB8DTHH",
        "outputId": "bcfba5c9-d53b-4fe1-dc7f-1ee7a6cb48b6"
      },
      "outputs": [],
      "source": [
        "print(f\"global_step = {global_step}, average loss = {tr_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCGyITCFHMFQ"
      },
      "source": [
        "# Evaluate\n",
        "각 checkpoint에서의 모델의 성능을 확인함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tman2JWCHLpH"
      },
      "outputs": [],
      "source": [
        "checkpoints = list(\n",
        "      os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PGQPudG3HfFw",
        "outputId": "80988bff-b164-4b23-d844-8741dbc5c7fc"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for checkpoint in checkpoints:\n",
        "  _gloabl_step = checkpoint.split(\"-\")[-1]\n",
        "  eval_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "  eval_model.to(args.device)\n",
        "  result = evaluate(eval_model, test_dataset, mode=\"test\", _global_step=_gloabl_step)\n",
        "  result = dict((k +f\"_{_gloabl_step}\", v) for k, v in result.items())\n",
        "  results.update(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cerZLuqkIYbS",
        "outputId": "deac2392-f4d5-4778-c851-5ec481b1214c"
      },
      "outputs": [],
      "source": [
        "for key in sorted(results.keys()):\n",
        "  print(f\"{key} = {results[key]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "soongsil-bert-base-nsmc.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
