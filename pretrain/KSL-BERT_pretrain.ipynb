{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                         1.15.2\n",
      "tensorflow-estimator               1.15.1\n",
      "tensorflow-gpu                     1.15.2\n",
      "tensorflow-serving-api             1.15.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 버전 확인\n",
    "!pip3 list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 29 23:21:42 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   31C    P0    27W / 250W |      0MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   29C    P0    26W / 250W |      0MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizers 설치 및 google-bert clone\n",
    "!pip install -q tokenizers\n",
    "!git clone -q https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print('GPU 사용여부: ', tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 학습\n",
    "\n",
    "- 전체 데이터 사용\n",
    "- vocab_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "tokenizer.train(['./data/dataset.txt'], vocab_size=30000, show_progress=True, limit_alphabet=3000)\n",
    "tokenizer.save_model('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 데이터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./shards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 분할\n",
    "# -a : 분할될 파일이름 접미사에 붙을 길이\n",
    "# -l : 라인 수 기준으로 파일 분할\n",
    "# -d : 분할될 파일이름과 저장될 파일이름 지정\n",
    "!split -a 3 -l 4800000 -d ./data/dataset.txt ./shards/shard_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./pretraining_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./shards/ | xargs -n 1 -P 4 -I{} python bert/create_pretraining_data.py --input_file=./shards/{} --output_file=./pretraining_data/{}.tfrecord --vocab_file=./data/vocab.txt --do_lower_case=False --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT MODEL CONFIG 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"bert_model\" \n",
    "tf.gfile.MkDir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1, \n",
    "    \"directionality\": \"bidi\", \n",
    "    \"hidden_act\": \"gelu\", \n",
    "    \"hidden_dropout_prob\": 0.1, \n",
    "    \"hidden_size\": 768, \n",
    "    \"initializer_range\": 0.02, \n",
    "    \"intermediate_size\": 3072, \n",
    "    \"max_position_embeddings\": 512, \n",
    "    \"num_attention_heads\": 12, \n",
    "    \"num_hidden_layers\": 12, \n",
    "    \"pooler_fc_size\": 768, \n",
    "    \"pooler_num_attention_heads\": 12, \n",
    "    \"pooler_num_fc_layers\": 3, \n",
    "    \"pooler_size_per_head\": 128, \n",
    "    \"pooler_type\": \"first_token_transform\", \n",
    "    \"type_vocab_size\": 2, \n",
    "    \"vocab_size\": 30000,\n",
    "    \"model_type\": \"bert-base\",\n",
    "    \"architectures\": [\"BertForMaskedLM\"]\n",
    "  }\n",
    "\n",
    "with open(\"{}/config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "    json.dump(bert_base_config, fo, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 사전학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트로 1만번 학습 \n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python bert/run_pretraining.py --input_file=../pretrain_test/src/pretraining_data/*.tfrecord --output_dir=./bert_model --do_train=True --do_eval=True --bert_config_file=./bert_model/config.json --train_batch_size=32 --max_seq_length=128 --max_predictions_per_seq=20 --num_train_steps=10000 --num_warmup_steps=10 --save_checkpoints_steps=1000 --learning_rate=2e-5"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60af5c81ffa00bed911704ff054405489da13f9503e86373e95cf9267d593cbf"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('tensorflow_p36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
