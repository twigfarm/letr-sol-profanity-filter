# KSL-BERT (Korean Spoken Language-BERT)

### í•œêµ­ì–´ êµ¬ì–´ì²´ ë¬¸ì¥ì— íŠ¹í™”ëœ ì‚¬ì „ í•™ìŠµ ëª¨ë¸

<br>
ê³µê°œëœ í•œêµ­ì–´ BERTëŠ” ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ ëŒ“ê¸€í˜• ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.

KcBERTëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ BERTëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.

**KSL-BERTëŠ” íŠ¹ì • ë„ë©”ì¸ì— êµ­í•œë˜ì§€ ì•Šìœ¼ë©° KcBERTì˜ ë°ì´í„°ì— ë”í•´ í•œêµ­ì–´ êµ¬ì–´ì²´ì˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ì²˜ìŒë¶€í„° í•™ìŠµí•œ BERT ëª¨ë¸ ê¸°ë°˜ì˜ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.**

**KSL-BERT**ì˜ Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.) <br>

## How to use

### Requirements

- `pytorch ~= 1.6.0`
- `transformers ~= 3.5.1`

```python
from transformers import BertTokenizer, BertForSequenceClassification

# Load Tokenizer
tokenizer = BertTokenizer.from_pretrained('dobbytk/KSL-BERT')

# Load Model
model = BertModel.from_pretrained('dobbytk/KSL-BERT')
```

## Train Data & Preprocessing

### Raw Data

1. KcBERT í•™ìŠµì„ ìœ„í•œ ë§ë­‰ì¹˜
2. êµ­ë¦½êµ­ì–´ì›, ì¼ìƒ ëŒ€í™” ë§ë­‰ì¹˜ (2020)
3. êµ­ë¦½êµ­ì–´ì›, ê°ì„± ë¶„ì„ ë§ë­‰ì¹˜ (2020)
4. êµ­ë¦½êµ­ì–´ì›, êµ¬ì–´ ë§ë­‰ì¹˜
5. íŠ¸ìœ„ê·¸íŒœ êµ¬ì–´ ë§ë­‰ì¹˜

ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ ì‹œ ì•½ 14GBì´ë©°, ì•½ 1ì–µ 2ì²œë§Œê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.

### Preprocessing

1. [KSS](https://github.com/hyunwoongko/kss) (Korean Sentence Segmentation Toolkit) ë¬¸ì¥ ìª¼ê°œê¸°
   - ë¬¸ì¥ì˜ í† í° ìˆ˜ê°€ BERT ëª¨ë¸ ì…ë ¥ ìµœëŒ€ í—ˆìš© ë²”ìœ„ (512 í† í°)ì„ ë„˜ì–´ê°€ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ì˜€ìŠµë‹ˆë‹¤.
2. Clean í•¨ìˆ˜ ì ìš©
   - ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, Emoji ğŸ˜Š ê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í•˜ì˜€ìŠµë‹ˆë‹¤.
   - ë¬¸ì¥ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½í•˜ì˜€ìŠµë‹ˆë‹¤. (ê°™ì€ ë¬¸ì 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ê²½ìš°)
     - ex) `ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹` -> `ã…‹ã…‹`

```shell
pip install soynlp emoji
```

```python
import re
import emoji
from soynlp.normalizer import repeat_normalize

emojis = list({y for x in emoji.UNICODE_EMOJI.values() for y in x.keys()})
emojis = ''.join(emojis)
pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-ã…£ê°€-í£{emojis}]+')
url_pattern = re.compile(
  r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
  x = pattern.sub(' ', x)
  x = url_pattern.sub('', x)
  x = x.strip()
  x = repeat_normalize(x, num_repeats=2)
  return x
```

3. Charê¸°ì¤€ 10ê°œ ì´í•˜ì¸ ë¬¸ì¥ ì œê±°
   - 10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤.
4. ê°ì„± ë¶„ì„ ë§ë­‰ì¹˜ì˜ ê²½ìš°, í•´ì‹œíƒœê·¸ ì œê±°
   - ê°ì„± ë¶„ì„ ë§ë­‰ì¹˜ì˜ ê²½ìš° SNS íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ í•´ì‹œíƒœê·¸(#)ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤.
5. ì¤‘ë³µ ì œê±°
   - ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ë¬¸ì¥ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì™„ì „ì¼ì¹˜ë°©ì‹ìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ë§Œë“  ìµœì¢… í•™ìŠµ ë°ì´í„°ëŠ” 13GB, ì•½ 9700ë§Œ ê°œ ë¬¸ì¥ì…ë‹ˆë‹¤.

## Tokenizer Train

TokenizerëŠ” Huggingfaceì˜ Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.<br>
ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000` ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.<br>
Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

## BERT Model Pretrain

- KSL-BERT Base config

```
{
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 30000,
  "model_type": "bert",
  "architectures": ["BertForMaskedLM"]
}
```

BERT Model Config ëŠ” Base ê¸°ë³¸ ì„¸íŒ…ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (MLM 15% ë“±)<br>
Tesla V100 GPUë¥¼ ì´ìš©í•˜ì—¬ ì•½ 20ì¼ë™ì•ˆ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggigfaceì— ê³µê°œëœ ëª¨ë¸ì€ 250ë§Œ stepì„ í•™ìŠµí•œ ckptê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

- KSL-BERT Base Loss
  ![KakaoTalk_Photo_2021-10-24-22-08-51](https://user-images.githubusercontent.com/51789449/138595627-32df7639-bd4d-4da5-a9ab-4c5cdaa5ab00.png)

ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 500Kê¹Œì§€ ê°€ì¥ ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë‹¤ 1Mì´í›„ë¡œëŠ” ì¡°ê¸ˆì”© ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Example

### HuggingFace MASK LM

[HuggingFace KSL-BERT Base ëª¨ë¸](https://huggingface.co/dobbytk/KSL-BERT) ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
![KakaoTalk_Image_2021-10-24-23-12-25](https://user-images.githubusercontent.com/51789449/138597972-169144db-df31-4fac-8755-b9b318ba0d9d.png)

![KakaoTalk_Image_2021-10-24-22-35-30](https://user-images.githubusercontent.com/51789449/138596570-18f19530-bbe9-41fa-be99-71a3ee0b30e8.png)

## KSL-BERT Performance

|                   | Size  | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) | **Korean-Hate-Speech (Dev)**<br/>(F1) |
| :---------------- | :---: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: | :-----------------------------------: |
| KoBERT            | 351M  |       89.59        |         87.92          |       81.25        |        79.62         |           81.59           |            94.85            |         51.75 / 79.15         |                 66.21                 |
| XLM-Roberta-Base  | 1.03G |       89.03        |         86.65          |       82.80        |        80.23         |           78.45           |            93.80            |         64.70 / 88.94         |                 64.06                 |
| HanBERT           | 614M  |       90.06        |         87.70          |       82.95        |        80.32         |           82.73           |            94.72            |         78.74 / 92.02         |                 68.32                 |
| KoELECTRA-Base-v3 | 431M  |       90.63        |         88.11          |       84.45        |        82.24         |           85.53           |            95.25            |         84.83 / 93.45         |                 67.61                 |
| Soongsil-BERT     | 370M  |      **91.2**      |           -            |         -          |          -           |            76             |             94              |               -               |                **69**                 |
| KSL-BERT          | 419M  |       89.24        |           -            |         -          |          -           |             -             |              -              |               -               |                 67.30                 |

## Reference

### Github Repos

- [BERT by Google](https://github.com/google-research/bert)
- [KcBERT by Beomi](https://github.com/Beomi/KcBERT)
- [Soongsil-BERT](https://github.com/jason9693/Soongsil-BERT)
- [KSS(korean sentence segmentation)](https://github.com/hyunwoongko/kss)
- [Transformers by Huggigface](https://github.com/huggingface/transformers)
- [Tokenizers by Huggingface](https://github.com/huggingface/tokenizers)

### Papers

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
